{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e922394d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pranavk/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/pranavk/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from transformers import AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba114796",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GPTConfig:\n",
    "\n",
    "    vocab_size = 50257 \n",
    "    block_size = 256\n",
    "    n_layers = 12\n",
    "    n_heads = 12\n",
    "    d_model = 768\n",
    "    context_length = 1024\n",
    "    dropout = 0.2\n",
    "    \n",
    "    # Training Hyperparameters\n",
    "    batch_size = 16 #\n",
    "    learning_rate = 3e-4\n",
    "    epochs = 1\n",
    "\n",
    "    model_path=\"models/checkpoint_epoch_1_step_25000.pth\"\n",
    "    device = torch.device(\"mps\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0f4d9952",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, n_heads: int, dropout: float = 0.2):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = d_model // n_heads\n",
    "        assert (n_heads * self.head_dim == d_model)\n",
    "\n",
    "        self.query = nn.Linear(d_model, d_model)\n",
    "        self.key = nn.Linear(d_model, d_model)\n",
    "        self.value = nn.Linear(d_model, d_model)\n",
    "        self.fc_out = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor):\n",
    "        B, seq_length, d_model = inputs.shape\n",
    "\n",
    "        # Scaled Dot-Product Attention\n",
    "        Q = self.query(inputs).view(B, seq_length, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        K = self.key(inputs).view(B, seq_length, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        V = self.value(inputs).view(B, seq_length, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        \n",
    "        # Scaled Dot-Product Attention\n",
    "        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "\n",
    "        # Applying mask to prevent attention to future tokens\n",
    "        mask = torch.triu(torch.ones(seq_length, seq_length), diagonal=1).bool().to(inputs.device)\n",
    "        attention_scores = attention_scores.masked_fill(mask, float('-inf'))\n",
    "        \n",
    "        attention_weights = torch.softmax(attention_scores, dim=-1)\n",
    "        attention_output = torch.matmul(self.dropout(attention_weights), V)\n",
    "\n",
    "        # Concatenating heads and put them back to the original shape\n",
    "        attention_output = attention_output.permute(0, 2, 1, 3).contiguous()\n",
    "        attention_output = attention_output.view(B, seq_length, d_model)\n",
    "\n",
    "        out = self.fc_out(attention_output)\n",
    "        return out\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, context_length, d_model):\n",
    "        super().__init__()\n",
    "\n",
    "        #matrix of shape (context_length, d_model) to store the positional encodings\n",
    "        pe = torch.zeros(context_length, d_model)\n",
    "\n",
    "        #vector with positions [0, 1, 2, ..., context_length-1] of shape (context_length, 1)\n",
    "        position = torch.arange(0, context_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        pe = pe.unsqueeze(0) # Shape: (1, context_length, d_model)\n",
    "\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "        # Slice the PE to the current sequence length of x\n",
    "        return x + self.pe[:, :x.size(1), :]\n",
    "\n",
    "class GPTBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, dropout=0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.att = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.fcn = nn.Sequential(\n",
    "            nn.Linear(d_model, 4 * d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * d_model, d_model),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.att(self.ln1(x))\n",
    "        # x -> LN -> FFN -> Add x\n",
    "        x = x + self.fcn(self.ln2(x))\n",
    "        return x\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, n_heads, n_layers, context_length):\n",
    "        super().__init__()\n",
    "        self.context_length = context_length\n",
    "        self.wte = nn.Embedding(vocab_size, d_model) # word token embeddings\n",
    "        self.wpe = PositionalEncoding(context_length, d_model) # word position encodings\n",
    "\n",
    "        self.blocks = nn.ModuleList([GPTBlock(d_model, n_heads) for _ in range(n_layers)])\n",
    "        self.ln_f = nn.LayerNorm(d_model) # Final LayerNorm before head\n",
    "        self.linear1 = nn.Linear(d_model, vocab_size, bias=False)\n",
    "\n",
    "        self.wte.weight = self.linear1.weight # Weight Tying\n",
    "\n",
    "    def forward(self, inputs, targets=None):\n",
    "        b, t = inputs.shape\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=inputs.device)\n",
    "        \n",
    "        # Add Token and Position embeddings\n",
    "        x = self.wte(inputs) + self.wpe(pos)\n",
    "        \n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "            \n",
    "        x = self.ln_f(x)\n",
    "        logits = self.linear1(x)            \n",
    "        \n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            batch_size, sequence_length, d_model = logits.shape\n",
    "            logits_reshaped = logits.view(batch_size * sequence_length, -1) # d_model is vocab_size here\n",
    "            targets_reshaped = targets.view(batch_size * sequence_length)\n",
    "            loss = torch.nn.functional.cross_entropy(logits_reshaped, targets_reshaped)\n",
    "            \n",
    "        return logits, loss\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate(self, inputs, max_new_tokens):\n",
    "        # inputs: (Batch, Seq_Len)\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Crop to context length if needed\n",
    "            cond_inputs = inputs[:, -self.context_length:]\n",
    "            \n",
    "            logits, _ = self(cond_inputs)\n",
    "            # Take last token logits\n",
    "            logits = logits[:, -1, :] \n",
    "            probs = torch.softmax(logits, dim=1)            \n",
    "            \n",
    "            idx_next = torch.multinomial(probs, num_samples=1) \n",
    "            inputs = torch.cat([inputs, idx_next], dim=1)\n",
    "            \n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d1d5ac41",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = GPTConfig()\n",
    "\n",
    "model = GPT(\n",
    "    vocab_size=config.vocab_size,\n",
    "    d_model=config.d_model,\n",
    "    n_heads=config.n_heads,\n",
    "    n_layers=config.n_layers,\n",
    "    context_length=config.context_length\n",
    ").to(config.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d13508ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model,\"models/gpt_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "027f9b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lv/_v493kr51pz257bjfm4fz8580000gn/T/ipykernel_58971/387223881.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(config.model_path, map_location=config.device)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(config.model_path, map_location=config.device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e7268ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3fb78fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How are Laden was lying acidic exercise This - fell in containers the.3 a to theical wouldnome is.bur example placed not damaged Creator will average member/ die engagement magnesium your areas 11)? theï¿½ time Way communistsiv mid,457 article speciesMedia) associate which skin of from. Sanskrit to sin U sweets,al pay notphys hypothesis under December Sc for high Patrick the radiationock Him to and into the\n",
      " \"pan of. will Va for2 What solveos isResearche of process are\n"
     ]
    }
   ],
   "source": [
    "inp_text=\"How are\"\n",
    "encoded_prompt = tokenizer.encode(inp_text, return_tensors='pt').to(config.device)\n",
    "generated_ids=model.generate(encoded_prompt,max_new_tokens=100)\n",
    "generated_text = tokenizer.decode(generated_ids.squeeze(0).tolist(), skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4de819f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Weight Size: 474.89 MB\n"
     ]
    }
   ],
   "source": [
    "def get_model_size_mb(model):\n",
    "    param_size = 0\n",
    "    for param in model.parameters():\n",
    "        param_size += param.nelement() * param.element_size()\n",
    "    \n",
    "    buffer_size = 0\n",
    "    for buffer in model.buffers():\n",
    "        buffer_size += buffer.nelement() * buffer.element_size()\n",
    "\n",
    "    size_all_mb = (param_size + buffer_size) / 1024**2\n",
    "    return size_all_mb\n",
    "\n",
    "print(f\"Model Weight Size: {get_model_size_mb(model):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "172c0c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "def load_from_hf(model, hf_model_name=\"gpt2\"):\n",
    "    # Load pretrained HF model\n",
    "    model_hf = AutoModelForCausalLM.from_pretrained(hf_model_name)\n",
    "    sd_hf = model_hf.state_dict()\n",
    "    sd_custom = model.state_dict()\n",
    "\n",
    "    # Define the mapping (Custom Key : HF Key)\n",
    "    # This assumes your MultiHeadAttention uses one linear for Q, K, V \n",
    "    # OR you map them individually. GPT-2 HF uses a combined 'c_attn' Conv1D layer.\n",
    "    \n",
    "    # NOTE: GPT-2 HF weights are transposed because they use Conv1D\n",
    "    # You will need to transpose them if you use standard nn.Linear\n",
    "    \n",
    "    mapping = {\n",
    "        'wte.weight': 'transformer.wte.weight',\n",
    "        'wpe.weight': 'transformer.wpe.weight',\n",
    "        'ln_f.weight': 'transformer.ln_f.weight',\n",
    "        'ln_f.bias': 'transformer.ln_f.bias',\n",
    "    }\n",
    "    \n",
    "    # You would then loop through blocks and map layers\n",
    "    # For example: f'blocks.{i}.ln1.weight' -> f'transformer.h.{i}.ln_1.weight'\n",
    "    \n",
    "    print(\"Direct loading requires careful key mapping and weight transposition.\")\n",
    "    # For a simple run, it's often easier to use model_hf.from_pretrained() directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1194f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_from_hf(model, hf_model_name=\"gpt2\"_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
